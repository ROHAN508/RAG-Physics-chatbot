{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15f5eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from semantic_text_splitter import TextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "# import faiss\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928b33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_from_folder(folder_path: str):\n",
    "    documents = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                loader = PyMuPDFLoader(file_path)\n",
    "                docs = loader.load()\n",
    "                \n",
    "                if not any(doc.page_content.strip() for doc in docs):\n",
    "                    raise ValueError(\"Empty text, fallback to OCR\")\n",
    "\n",
    "            except Exception:\n",
    "                print(f\"couldent load {file}\")\n",
    "            documents.extend(docs)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8c1bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldent load allen H.B - Thermodynamics And An Introduction To Thermostatistics-Wiley (1985).pdf\n",
      "couldent load BMP notes.pdf\n",
      "couldent load classical-mechanics-j-c-upadhyay-2014-edition.pdf\n",
      "couldent load Classical_Electrodynamics_Jackson_1a_Edition.pdf\n",
      "couldent load dummit-amp-footex27s-algebra-pr_ff0ec52449105ac0359a55c3c10fbbd3.pdf\n",
      "couldent load Francis A. Jenkins, Harvey E. White - Fundamentals of Optics, Fourth Edition     (2001, McGraw-Hill_.pdf\n",
      "couldent load Griffiths - Introduction to quantum mechanics.pdf\n",
      "couldent load real-analysis-by-bartle.pdf\n",
      "couldent load the-feynman-lectures-on-physics-vol2-pr_535b8c931c7110f49cd675c08ed78b62.pdf\n",
      "couldent load the-feynman-lectures-on-physics-vol3-pr_669bff2604e3c5241128c53adcb99a0d.pdf\n",
      "MuPDF error: library error: FT_New_Memory_Face(GOOEBM+MSBM10): unknown file format\n",
      "\n",
      "MuPDF error: library error: FT_New_Memory_Face(GPFBMM+MSBM7): unknown file format\n",
      "\n",
      "MuPDF error: library error: FT_New_Memory_Face(GPHPAJ+stmary7): unknown file format\n",
      "\n",
      "MuPDF error: library error: FT_New_Memory_Face(GPHPBK+stmary10): unknown file format\n",
      "\n",
      "MuPDF error: library error: FT_New_Memory_Face(GPIACF+stmary6): unknown file format\n",
      "\n",
      "MuPDF error: library error: FT_New_Memory_Face(GPIADF+stmary5): unknown file format\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = load_pdf_from_folder(\"D:\\Machine Learning Practice\\Pdf data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b3b283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='84\n",
      "CHAPTER 2. MATHEMATICAL TOOLS OF QUANTUM MECHANICS\n",
      "2.2.4\n",
      "Square-Integrable Functions: Wave Functions\n",
      "In the case of function spaces, a “vector” element is given by a complex function and the scalar\n",
      "product by integrals. That is, the scalar product of two functions O\u0010x\u0011 and M\u0010x\u0011 is given by\n",
      "\u0010O\u001d M\u0011 \u0007\n",
      "=\n",
      "O`\u0010x\u0011M\u0010x\u0011 dx\u001c\n",
      "(2.21)\n",
      "If this integral diverges, the scalar product does not exist. As a result, if we want the function\n",
      "space to possess a scalar product, we must select only those functions for which \u0010O\u001d M\u0011 is ﬁnite.\n",
      "In particular, a function O\u0010x\u0011 is said to be square integrable if the scalar product of O with\n",
      "itself,\n",
      "\u0010O\u001d O\u0011 \u0007\n",
      "=\n",
      "\u000bO\u0010x\u0011\u000b2 dx\u001d\n",
      "(2.22)\n",
      "is ﬁnite.\n",
      "It is easy to verify that the space of square-integrable functions possesses the properties of\n",
      "a Hilbert space. For instance, any linear combination of square-integrable functions is also a\n",
      "square-integrable function and (2.21) satisﬁes all the properties of the scalar product of a Hilbert\n",
      "space.\n",
      "Note that the dimension of the Hilbert space of square-integrable functions is inﬁnite, since\n",
      "each wave function can be expanded in terms of an inﬁnite number of linearly independent\n",
      "functions. The dimension of a space is given by the maximum number of linearly independent\n",
      "basis vectors required to span that space.\n",
      "A good example of square-integrable functions is the wave function of quantum mechanics,\n",
      "O\u0010;r\u001d t\u0011. We have seen in Chapter 1 that, according to Born’s probabilistic interpretation of\n",
      "O\u0010;r\u001d t\u0011, the quantity \u000b O\u0010;r\u001d t\u0011 \u000b2 d3r represents the probability of ﬁnding, at time t, the particle\n",
      "in a volume d3r, centered around the point ;r. The probability of ﬁnding the particle somewhere\n",
      "in space must then be equal to 1:\n",
      "=\n",
      "\u000b O\u0010;r\u001d t\u0011 \u000b2 d3r \u0007\n",
      "= \u0005*\n",
      "\u0013*\n",
      "dx\n",
      "= \u0005*\n",
      "\u0013*\n",
      "dy\n",
      "= \u0005*\n",
      "\u0013*\n",
      "\u000b O\u0010;r\u001d t\u0011 \u000b2 dz \u0007 1\u0006\n",
      "(2.23)\n",
      "hence the wave functions of quantum mechanics are square-integrable. Wave functions sat-\n",
      "isfying (2.23) are said to be normalized or square-integrable. As wave mechanics deals with\n",
      "square-integrable functions, any wave function which is not square-integrable has no physical\n",
      "meaning in quantum mechanics.\n",
      "2.3\n",
      "Dirac Notation\n",
      "The physical state of a system is represented in quantum mechanics by elements of a Hilbert\n",
      "space; these elements are called state vectors. We can represent the state vectors in different\n",
      "bases by means of function expansions. This is analogous to specifying an ordinary (Euclid-\n",
      "ean) vector by its components in various coordinate systems. For instance, we can represent\n",
      "equivalently a vector by its components in a Cartesian coordinate system, in a spherical coor-\n",
      "dinate system, or in a cylindrical coordinate system. The meaning of a vector is, of course,\n",
      "independent of the coordinate system chosen to represent its components. Similarly, the state\n",
      "of a microscopic system has a meaning independent of the basis in which it is expanded.\n",
      "To free state vectors from coordinate meaning, Dirac introduced what was to become an in-\n",
      "valuable notation in quantum mechanics; it allows one to manipulate the formalism of quantum' metadata={'producer': 'itext-paulo-155 (itextpdf.sf.net-lowagie.com)', 'creator': 'pdftk 1.41 - www.pdftk.com', 'creationdate': '2009-04-09T14:25:39+01:00', 'source': 'D:\\\\Machine Learning Practice\\\\Pdf data\\\\02110tpnews_11232020.pdf', 'file_path': 'D:\\\\Machine Learning Practice\\\\Pdf data\\\\02110tpnews_11232020.pdf', 'total_pages': 691, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2009-04-09T14:25:39+01:00', 'trapped': '', 'modDate': \"D:20090409142539+01'00'\", 'creationDate': \"D:20090409142539+01'00'\", 'page': 100}\n"
     ]
    }
   ],
   "source": [
    "print(docs[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bade8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size = 1000, chunk_overlap = 200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap, separators= [\"\\n\\n\", \"\\n\", \" \", \"\"] )\n",
    "    split = text_splitter.split_documents(documents=documents)\n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0244c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c5914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40774\n"
     ]
    }
   ],
   "source": [
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4b22a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunks 0 to 500\n",
      "Processed chunks 500 to 1000\n",
      "Processed chunks 1000 to 1500\n",
      "Processed chunks 1500 to 2000\n",
      "Processed chunks 2000 to 2500\n",
      "Processed chunks 2500 to 3000\n",
      "Processed chunks 3000 to 3500\n",
      "Processed chunks 3500 to 4000\n",
      "Processed chunks 4000 to 4500\n",
      "Processed chunks 4500 to 5000\n",
      "Processed chunks 5000 to 5500\n",
      "Processed chunks 5500 to 6000\n",
      "Processed chunks 6000 to 6500\n",
      "Processed chunks 6500 to 7000\n",
      "Processed chunks 7000 to 7500\n",
      "Processed chunks 7500 to 8000\n",
      "Processed chunks 8000 to 8500\n",
      "Processed chunks 8500 to 9000\n",
      "Processed chunks 9000 to 9500\n",
      "Processed chunks 9500 to 10000\n",
      "Processed chunks 10000 to 10500\n",
      "Processed chunks 10500 to 11000\n",
      "Processed chunks 11000 to 11500\n",
      "Processed chunks 11500 to 12000\n",
      "Processed chunks 12000 to 12500\n",
      "Processed chunks 12500 to 13000\n",
      "Processed chunks 13000 to 13500\n",
      "Processed chunks 13500 to 14000\n",
      "Processed chunks 14000 to 14500\n",
      "Processed chunks 14500 to 15000\n",
      "Processed chunks 15000 to 15500\n",
      "Processed chunks 15500 to 16000\n",
      "Processed chunks 16000 to 16500\n",
      "Processed chunks 16500 to 17000\n",
      "Processed chunks 17000 to 17500\n",
      "Processed chunks 17500 to 18000\n",
      "Processed chunks 18000 to 18500\n",
      "Processed chunks 18500 to 19000\n",
      "Processed chunks 19000 to 19500\n",
      "Processed chunks 19500 to 20000\n",
      "Processed chunks 20000 to 20500\n",
      "Processed chunks 20500 to 21000\n",
      "Processed chunks 21000 to 21500\n",
      "Processed chunks 21500 to 22000\n",
      "Processed chunks 22000 to 22500\n",
      "Processed chunks 22500 to 23000\n",
      "Processed chunks 23000 to 23500\n",
      "Processed chunks 23500 to 24000\n",
      "Processed chunks 24000 to 24500\n",
      "Processed chunks 24500 to 25000\n",
      "Processed chunks 25000 to 25500\n",
      "Processed chunks 25500 to 26000\n",
      "Processed chunks 26000 to 26500\n",
      "Processed chunks 26500 to 27000\n",
      "Processed chunks 27000 to 27500\n",
      "Processed chunks 27500 to 28000\n",
      "Processed chunks 28000 to 28500\n",
      "Processed chunks 28500 to 29000\n",
      "Processed chunks 29000 to 29500\n",
      "Processed chunks 29500 to 30000\n",
      "Processed chunks 30000 to 30500\n",
      "Processed chunks 30500 to 31000\n",
      "Processed chunks 31000 to 31500\n",
      "Processed chunks 31500 to 32000\n",
      "Processed chunks 32000 to 32500\n",
      "Processed chunks 32500 to 33000\n",
      "Processed chunks 33000 to 33500\n",
      "Processed chunks 33500 to 34000\n",
      "Processed chunks 34000 to 34500\n",
      "Processed chunks 34500 to 35000\n",
      "Processed chunks 35000 to 35500\n",
      "Processed chunks 35500 to 36000\n",
      "Processed chunks 36000 to 36500\n",
      "Processed chunks 36500 to 37000\n",
      "Processed chunks 37000 to 37500\n",
      "Processed chunks 37500 to 38000\n",
      "Processed chunks 38000 to 38500\n",
      "Processed chunks 38500 to 39000\n",
      "Processed chunks 39000 to 39500\n",
      "Processed chunks 39500 to 40000\n",
      "Processed chunks 40000 to 40500\n",
      "Processed chunks 40500 to 40774\n",
      "FAISS vector store saved successfully!\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",model_kwargs={\"device\": \"cuda\"})\n",
    "# vectordb = Chroma.from_documents(documents=chunks, embedding= embedding_model,persist_directory=\"./chroma_db\")\n",
    "# vectordb = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_model)\n",
    "# batch_size =64\n",
    "# for i in range(0, len(chunks),batch_size):\n",
    "#     batch = chunks[i:i+batch_size]\n",
    "#     try:\n",
    "#         vectordb.add_documents(batch)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error adding batch {i} to {i+batch_size}: {e}\")\n",
    "#         break\n",
    "# vectordb.persist()\n",
    "\n",
    "# vectordb = FAISS.from_documents(chunks, embedding_model)\n",
    "# FAISS.save_local(vectordb, \"./faiss_db\")\n",
    "\n",
    "batch_size = 500  # safe batch size\n",
    "vectordb = None\n",
    "for i in range(0, len(chunks), batch_size):\n",
    "    batch = chunks[i:i+batch_size]\n",
    "    if vectordb is None:\n",
    "        vectordb = FAISS.from_documents(batch, embedding_model)\n",
    "    else:\n",
    "        vectordb.add_documents(batch)\n",
    "    print(f\"Processed chunks {i} to {i + len(batch)}\")\n",
    "\n",
    "FAISS.save_local(vectordb, \"./faiss_db\")\n",
    "print(\"FAISS vector store saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
